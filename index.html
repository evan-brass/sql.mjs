<!DOCTYPE html>
<script type="_importmap">{ "imports": {
	"sql.mjs": "https://unpkg.com/sql.mjs",
	"sql.mjs/": "https://unpkg.com/sql.mjs/"
}}</script>
<script type="importmap">{ "imports": {
	"sql.mjs": "/dist/index.mjs",
	"sql.mjs/": "/dist/"
}}</script>
<script type="module">
	import {
		ConnPool, OpenParams, exec, register_vfs
	} from 'sql.mjs';
	import local from 'sql.mjs/vfs/local.mjs';
	import { StreamHandle } from 'sql.mjs/func/blob_io.mjs';

	const pool = new ConnPool(new OpenParams({pathname: 'opfs/blob-testing.db'}), {
		async initialized() {
			register_vfs(local, 1);
			const opfs_root = await navigator.storage.getDirectory();
			local.mount(opfs_root, '/opfs');
		}
	});

	// Run the schema:
	await pool.borrow(async conn => {
		await exec(conn.sql`
			CREATE TABLE IF NOT EXISTS files (
				filename TEXT NOT NULL UNIQUE,
				type TEXT DEFAULT 'text/plain',
				data BLOB NOT NULL
			) STRICT;
		`);
	});

	// Upload interface:
	const upload = document.createElement('button');
	upload.innerText = 'Upload';
	upload.onclick = async () => {
		// Show the file picker:
		const files = await showOpenFilePicker({
			multiple: true,
			// Allow all mime types
		});
		await pool.borrow(async conn => {
			// Start the (write) transaction:
			await exec(conn.sql`BEGIN IMMEDIATE;`);

			for (const file of files) {
				const blob = await file.getFile();
				const [bytes_written] = await exec(conn.sql`
					-- Reserve space for the file's data using a zero-blob
					INSERT INTO files VALUES (${blob.name}, ${blob.type}, zeroblob(${blob.size}));
					-- Stream the file's data into the blob
					--   blob_io() will use incremental IO to write the stream into the blob
					SELECT blob_io(${new StreamHandle({
						stream: blob.stream(), // The ReadableStream of the File
						buffer_size: 32_768 // The default buffer size is 4kb but we can manually adjust it
					})}, last_insert_rowid(), 'files', 'data');
				`);
				console.log('Wrote a file:', blob, 'bytes written', bytes_written);
			}

			await exec(conn.sql`COMMIT;`);
		});
	};
	document.body.append(upload);

	// List / View interface:
	const obj_urls = new Set();
	const table = document.createElement('table');
	table.innerHTML = `
		<thead>
			<tr>
				<th>Filename</th>
				<th>Content Type</th>
				<th>Length</th>
			</tr>
		</thead>
	`;
	const tbody = document.createElement('tbody');
	table.append(tbody);
	const list = document.createElement('button');
	list.innerText = 'List Files';
	list.onclick = () => pool.borrow(async conn => {
		tbody.innerHTML = '';
		for (const url of obj_urls) {
			URL.revokeObjectURL(url);
		}
		
		await exec(conn.sql`BEGIN;`);
		for await (const {rowid, filename, type, size} of conn.sql`SELECT rowid, filename, type, length(data) AS size FROM files;`) {
			// Create a blob and then object URL for the file:
			const blob = await (async () => {
				const ts = new TransformStream();
	
				const res = new Response(ts.readable, { headers: {
					'Content-Type': type,
				}});
	
				// Simultaneously Turn the response into a js Blob and stream the SQLite Blob into the request.
				const [ret] = await Promise.all([
					res.blob(),
					exec(conn.sql`SELECT blob_io(${ts.writable}, ${rowid}, 'files', 'data');`)
				]);
				return ret;
			})();

			const obj_url = URL.createObjectURL(blob);
			obj_urls.add(obj_url);

			// Live on the edge and don't escape your html!
			tbody.insertAdjacentHTML('beforeend',
				`<tr>
					<td><a href="${obj_url}">${filename}</a></td>
					<td>${type}</td>
					<td>${size}</td>
				</tr>`
			);
		}
		await exec(conn.sql`COMMIT;`);
	});
	document.body.append(table, list);
</script>
